{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICU Vital Sign Forecasting - Data Preprocessing & Feature Engineering\n",
    "\n",
    "**Project:** Forecasting ICU Patient Vital-Sign Deterioration Using Hybrid Statistical and Deep Learning Models\n",
    "\n",
    "**This Notebook Covers:**\n",
    "1. Data Cleaning (outlier removal, validation)\n",
    "2. Temporal Resampling (irregular → regular intervals)\n",
    "3. Missing Value Imputation\n",
    "4. Normalization/Standardization\n",
    "5. Feature Engineering (rolling stats, rate of change)\n",
    "6. Windowed Sequence Creation (input → forecast)\n",
    "7. Train/Validation/Test Split\n",
    "8. Deterioration Label Creation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths configured\n"
     ]
    }
   ],
   "source": [
    "## INSERT PATH FROM YOUR DESTINATION\n",
    "BASE_PATH = r\"C:\\Users\\hasin\\Downloads\\mimic-iii-clinical-database-1.4\\mimic-iii-clinical-database-1.4\"\n",
    "\n",
    "# If you saved the extracted vitals from EDA notebook:\n",
    "VITALS_PATH = f\"{BASE_PATH}\\\\vitals_sample_2000.csv\"\n",
    "ICUSTAYS_PATH = f\"{BASE_PATH}\\\\ICUSTAYS.csv\\\\ICUSTAYS.csv\"\n",
    "ADMISSIONS_PATH = f\"{BASE_PATH}\\\\ADMISSIONS.csv\\\\ADMISSIONS.csv\"\n",
    "PATIENTS_PATH = f\"{BASE_PATH}\\\\PATIENTS.csv\\\\PATIENTS.csv\"\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_PATH = f\"{BASE_PATH}\\\\processed\"\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"Paths configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vitals loaded from saved file: 1,663,885 records\n",
      "ICUSTAYS: 61,532 rows\n",
      "ADMISSIONS: 58,976 rows\n",
      "PATIENTS: 46,520 rows\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# Load vitals (from EDA notebook output or reload from CHARTEVENTS)\n",
    "try:\n",
    "    vitals_df = pd.read_csv(VITALS_PATH)\n",
    "    print(f\"Vitals loaded from saved file: {len(vitals_df):,} records\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Saved vitals file not found. Please run the EDA notebook first to extract vitals.\")\n",
    "\n",
    "# Load supporting tables\n",
    "icustays = pd.read_csv(ICUSTAYS_PATH)\n",
    "admissions = pd.read_csv(ADMISSIONS_PATH)\n",
    "patients = pd.read_csv(PATIENTS_PATH)\n",
    "\n",
    "print(f\"ICUSTAYS: {len(icustays):,} rows\")\n",
    "print(f\"ADMISSIONS: {len(admissions):,} rows\")\n",
    "print(f\"PATIENTS: {len(patients):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime columns converted\n",
      "\n",
      "Vitals date range: 2100-08-08 08:30:00 to 2205-10-15 22:00:00\n"
     ]
    }
   ],
   "source": [
    "# Convert datetime columns\n",
    "vitals_df['CHARTTIME'] = pd.to_datetime(vitals_df['CHARTTIME'])\n",
    "icustays['INTIME'] = pd.to_datetime(icustays['INTIME'])\n",
    "icustays['OUTTIME'] = pd.to_datetime(icustays['OUTTIME'])\n",
    "admissions['DEATHTIME'] = pd.to_datetime(admissions['DEATHTIME'])\n",
    "\n",
    "print(\"Datetime columns converted\")\n",
    "print(f\"\\nVitals date range: {vitals_df['CHARTTIME'].min()} to {vitals_df['CHARTTIME'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vital types assigned:\n",
      "VITAL_TYPE\n",
      "HR        335685\n",
      "RR        268656\n",
      "SBP       248313\n",
      "DBP       247554\n",
      "MBP       247252\n",
      "SpO2      241829\n",
      "Temp_C     53495\n",
      "Temp_F     21101\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define ITEMID mappings (from EDA)\n",
    "VITAL_ITEMIDS = {\n",
    "    'HR': [211, 220045],\n",
    "    'SpO2': [646, 220277],\n",
    "    'SBP': [51, 442, 455, 220179, 220050],\n",
    "    'DBP': [8368, 8440, 8441, 220180, 220051],\n",
    "    'MBP': [456, 52, 6702, 220052, 220181],\n",
    "    'RR': [618, 615, 220210, 224690],\n",
    "    'Temp_C': [223761, 678],\n",
    "    'Temp_F': [223762, 679]\n",
    "}\n",
    "\n",
    "# Physiologically plausible ranges\n",
    "VITAL_RANGES = {\n",
    "    'HR': (20, 300),\n",
    "    'SpO2': (50, 100),\n",
    "    'SBP': (40, 300),\n",
    "    'DBP': (20, 200),\n",
    "    'MBP': (30, 250),\n",
    "    'RR': (4, 60),\n",
    "    'Temp_C': (30, 45),\n",
    "    'Temp_F': (86, 113)\n",
    "}\n",
    "\n",
    "# Map ITEMIDs to vital names\n",
    "def map_vital_name(itemid):\n",
    "    for vital_name, itemids in VITAL_ITEMIDS.items():\n",
    "        if itemid in itemids:\n",
    "            return vital_name\n",
    "    return 'Other'\n",
    "\n",
    "# Add vital type column if not present\n",
    "if 'VITAL_TYPE' not in vitals_df.columns:\n",
    "    vitals_df['VITAL_TYPE'] = vitals_df['ITEMID'].apply(map_vital_name)\n",
    "\n",
    "print(\"Vital types assigned:\")\n",
    "print(vitals_df['VITAL_TYPE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA CLEANING\n",
      "============================================================\n",
      "\n",
      "Initial records: 1,663,885\n",
      "After removing nulls: 1,662,064 (1,821 removed)\n",
      "\n",
      "Outliers removed by vital type:\n",
      "  HR: 91 records\n",
      "  SpO2: 153 records\n",
      "  SBP: 578 records\n",
      "  DBP: 665 records\n",
      "  MBP: 653 records\n",
      "  RR: 1,486 records\n",
      "  Temp_C: 53,359 records\n",
      "  Temp_F: 3,767 records\n",
      "\n",
      "Final clean records: 1,601,312\n"
     ]
    }
   ],
   "source": [
    "# Clean data: Remove physiologically implausible values\n",
    "print(\"=\"*60)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert VALUENUM to numeric\n",
    "vitals_df['VALUENUM'] = pd.to_numeric(vitals_df['VALUENUM'], errors='coerce')\n",
    "\n",
    "initial_count = len(vitals_df)\n",
    "print(f\"\\nInitial records: {initial_count:,}\")\n",
    "\n",
    "# Remove null values\n",
    "vitals_df = vitals_df.dropna(subset=['VALUENUM'])\n",
    "print(f\"After removing nulls: {len(vitals_df):,} ({initial_count - len(vitals_df):,} removed)\")\n",
    "\n",
    "# Remove out-of-range values\n",
    "def remove_outliers(df):\n",
    "    clean_df = df.copy()\n",
    "    removed_counts = {}\n",
    "    \n",
    "    for vital, (low, high) in VITAL_RANGES.items():\n",
    "        mask = clean_df['VITAL_TYPE'] == vital\n",
    "        before = mask.sum()\n",
    "        \n",
    "        # Mark invalid values\n",
    "        invalid_mask = mask & ((clean_df['VALUENUM'] < low) | (clean_df['VALUENUM'] > high))\n",
    "        removed_counts[vital] = invalid_mask.sum()\n",
    "        \n",
    "        # Remove invalid\n",
    "        clean_df = clean_df[~invalid_mask]\n",
    "    \n",
    "    return clean_df, removed_counts\n",
    "\n",
    "vitals_clean, removed = remove_outliers(vitals_df)\n",
    "\n",
    "print(\"\\nOutliers removed by vital type:\")\n",
    "for vital, count in removed.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {vital}: {count:,} records\")\n",
    "\n",
    "print(f\"\\nFinal clean records: {len(vitals_clean):,}\")\n",
    "vitals_df = vitals_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Combining Temperature Variables ---\n",
      "Temperature unified (all in Celsius)\n",
      "VITAL_TYPE\n",
      "HR      335532\n",
      "RR      266961\n",
      "SBP     247103\n",
      "DBP     246889\n",
      "MBP     246047\n",
      "SpO2    241576\n",
      "Temp     17204\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combine Temperature (C and F) into single variable\n",
    "print(\"\\n--- Combining Temperature Variables ---\")\n",
    "\n",
    "# Convert Fahrenheit to Celsius\n",
    "temp_f_mask = vitals_df['VITAL_TYPE'] == 'Temp_F'\n",
    "vitals_df.loc[temp_f_mask, 'VALUENUM'] = (vitals_df.loc[temp_f_mask, 'VALUENUM'] - 32) * 5/9\n",
    "vitals_df.loc[temp_f_mask, 'VITAL_TYPE'] = 'Temp'\n",
    "\n",
    "# Rename Temp_C to Temp\n",
    "vitals_df.loc[vitals_df['VITAL_TYPE'] == 'Temp_C', 'VITAL_TYPE'] = 'Temp'\n",
    "\n",
    "print(\"Temperature unified (all in Celsius)\")\n",
    "print(vitals_df['VITAL_TYPE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Resampling (Hourly Aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEMPORAL RESAMPLING\n",
      "============================================================\n",
      "Records within ICU stay: 1,594,852\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TEMPORAL RESAMPLING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Add ICU stay information\n",
    "vitals_df = vitals_df.merge(\n",
    "    icustays[['ICUSTAY_ID', 'INTIME', 'OUTTIME', 'LOS']],\n",
    "    on='ICUSTAY_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate hours since ICU admission\n",
    "vitals_df['HOURS_IN'] = (vitals_df['CHARTTIME'] - vitals_df['INTIME']).dt.total_seconds() / 3600\n",
    "\n",
    "# Filter: Only keep measurements during ICU stay (0 to LOS hours)\n",
    "vitals_df = vitals_df[(vitals_df['HOURS_IN'] >= 0) & (vitals_df['HOURS_IN'] <= vitals_df['LOS'] * 24)]\n",
    "\n",
    "print(f\"Records within ICU stay: {len(vitals_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregating to hourly means...\n",
      "Hourly aggregated records: 1,266,615\n"
     ]
    }
   ],
   "source": [
    "# Create hour bins for resampling\n",
    "vitals_df['HOUR_BIN'] = vitals_df['HOURS_IN'].apply(lambda x: int(x))\n",
    "\n",
    "# Aggregate to hourly means\n",
    "print(\"\\nAggregating to hourly means...\")\n",
    "\n",
    "vitals_hourly = vitals_df.groupby(\n",
    "    ['ICUSTAY_ID', 'SUBJECT_ID', 'HADM_ID', 'HOUR_BIN', 'VITAL_TYPE']\n",
    ")['VALUENUM'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "vitals_hourly.columns = ['ICUSTAY_ID', 'SUBJECT_ID', 'HADM_ID', 'HOUR', 'VITAL_TYPE', 'VALUE', 'STD', 'COUNT']\n",
    "\n",
    "print(f\"Hourly aggregated records: {len(vitals_hourly):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivoting to wide format...\n",
      "Wide format shape: (286288, 12)\n",
      "Columns: ['ICUSTAY_ID', 'SUBJECT_ID', 'HADM_ID', 'HOUR', 'DBP', 'HR', 'MBP', 'RR', 'SBP', 'SpO2', 'Temp_C', 'Temp_F']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ICUSTAY_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>DBP</th>\n",
       "      <th>HR</th>\n",
       "      <th>MBP</th>\n",
       "      <th>RR</th>\n",
       "      <th>SBP</th>\n",
       "      <th>SpO2</th>\n",
       "      <th>Temp_C</th>\n",
       "      <th>Temp_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200006.000</td>\n",
       "      <td>10950</td>\n",
       "      <td>189514</td>\n",
       "      <td>1</td>\n",
       "      <td>87.000</td>\n",
       "      <td>81.000</td>\n",
       "      <td>97.333</td>\n",
       "      <td>21.333</td>\n",
       "      <td>118.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200006.000</td>\n",
       "      <td>10950</td>\n",
       "      <td>189514</td>\n",
       "      <td>2</td>\n",
       "      <td>64.000</td>\n",
       "      <td>73.000</td>\n",
       "      <td>79.333</td>\n",
       "      <td>14.000</td>\n",
       "      <td>110.000</td>\n",
       "      <td>99.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200006.000</td>\n",
       "      <td>10950</td>\n",
       "      <td>189514</td>\n",
       "      <td>3</td>\n",
       "      <td>53.000</td>\n",
       "      <td>78.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>101.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200006.000</td>\n",
       "      <td>10950</td>\n",
       "      <td>189514</td>\n",
       "      <td>4</td>\n",
       "      <td>50.000</td>\n",
       "      <td>77.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>99.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200006.000</td>\n",
       "      <td>10950</td>\n",
       "      <td>189514</td>\n",
       "      <td>5</td>\n",
       "      <td>58.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>76.667</td>\n",
       "      <td>24.500</td>\n",
       "      <td>114.000</td>\n",
       "      <td>97.500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ICUSTAY_ID  SUBJECT_ID  HADM_ID  HOUR    DBP     HR    MBP     RR     SBP  \\\n",
       "0  200006.000       10950   189514     1 87.000 81.000 97.333 21.333 118.000   \n",
       "1  200006.000       10950   189514     2 64.000 73.000 79.333 14.000 110.000   \n",
       "2  200006.000       10950   189514     3 53.000 78.000 69.000 18.000 101.000   \n",
       "3  200006.000       10950   189514     4 50.000 77.000 65.000 20.000  95.000   \n",
       "4  200006.000       10950   189514     5 58.000 80.000 76.667 24.500 114.000   \n",
       "\n",
       "     SpO2  Temp_C  Temp_F  \n",
       "0 100.000     NaN     NaN  \n",
       "1  99.000     NaN     NaN  \n",
       "2 100.000     NaN     NaN  \n",
       "3  99.000     NaN     NaN  \n",
       "4  97.500     NaN     NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivot to wide format (one column per vital)\n",
    "print(\"\\nPivoting to wide format...\")\n",
    "\n",
    "vitals_wide = vitals_hourly.pivot_table(\n",
    "    index=['ICUSTAY_ID', 'SUBJECT_ID', 'HADM_ID', 'HOUR'],\n",
    "    columns='VITAL_TYPE',\n",
    "    values='VALUE',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "vitals_wide.columns.name = None\n",
    "\n",
    "print(f\"Wide format shape: {vitals_wide.shape}\")\n",
    "print(f\"Columns: {list(vitals_wide.columns)}\")\n",
    "vitals_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating complete hourly grid...\n",
      "Complete timeline shape: (307210, 12)\n",
      "Total hours: 307,210\n"
     ]
    }
   ],
   "source": [
    "# Create complete hourly time series for each ICU stay\n",
    "print(\"\\nCreating complete hourly grid...\")\n",
    "\n",
    "def create_complete_timeline(group):\n",
    "    \"\"\"Fill in missing hours for each ICU stay\"\"\"\n",
    "    icustay_id = group['ICUSTAY_ID'].iloc[0]\n",
    "    subject_id = group['SUBJECT_ID'].iloc[0]\n",
    "    hadm_id = group['HADM_ID'].iloc[0]\n",
    "    \n",
    "    min_hour = int(group['HOUR'].min())\n",
    "    max_hour = int(group['HOUR'].max())\n",
    "    \n",
    "    # Create complete hour range\n",
    "    all_hours = pd.DataFrame({'HOUR': range(min_hour, max_hour + 1)})\n",
    "    all_hours['ICUSTAY_ID'] = icustay_id\n",
    "    all_hours['SUBJECT_ID'] = subject_id\n",
    "    all_hours['HADM_ID'] = hadm_id\n",
    "    \n",
    "    # Merge with existing data\n",
    "    complete = all_hours.merge(group, on=['ICUSTAY_ID', 'SUBJECT_ID', 'HADM_ID', 'HOUR'], how='left')\n",
    "    \n",
    "    return complete\n",
    "\n",
    "# Apply to each ICU stay\n",
    "vitals_complete = vitals_wide.groupby('ICUSTAY_ID', group_keys=False).apply(create_complete_timeline)\n",
    "vitals_complete = vitals_complete.reset_index(drop=True)\n",
    "\n",
    "print(f\"Complete timeline shape: {vitals_complete.shape}\")\n",
    "print(f\"Total hours: {len(vitals_complete):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISSING VALUE IMPUTATION\n",
      "============================================================\n",
      "\n",
      "Missingness before imputation:\n",
      "  HR: 22,218 (7.2%)\n",
      "  SpO2: 114,612 (37.3%)\n",
      "  SBP: 113,662 (37.0%)\n",
      "  DBP: 113,750 (37.0%)\n",
      "  MBP: 114,663 (37.3%)\n",
      "  RR: 110,774 (36.1%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"MISSING VALUE IMPUTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define vital columns\n",
    "VITAL_COLS = ['HR', 'SpO2', 'SBP', 'DBP', 'MBP', 'RR', 'Temp']\n",
    "VITAL_COLS = [col for col in VITAL_COLS if col in vitals_complete.columns]\n",
    "\n",
    "# Check missingness before imputation\n",
    "print(\"\\nMissingness before imputation:\")\n",
    "for col in VITAL_COLS:\n",
    "    missing = vitals_complete[col].isna().sum()\n",
    "    pct = missing / len(vitals_complete) * 100\n",
    "    print(f\"  {col}: {missing:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values...\n",
      "\n",
      "Missingness after imputation:\n",
      "  HR: 1,145 (0.4%)\n",
      "  SpO2: 97,072 (31.6%)\n",
      "  SBP: 97,413 (31.7%)\n",
      "  DBP: 97,412 (31.7%)\n",
      "  MBP: 97,742 (31.8%)\n",
      "  RR: 96,947 (31.6%)\n"
     ]
    }
   ],
   "source": [
    "# Imputation strategy:\n",
    "# 1. Forward fill (carry last observation forward) - up to 4 hours\n",
    "# 2. Backward fill (for initial missing values) - up to 4 hours\n",
    "# 3. Linear interpolation for remaining gaps - up to 6 hours\n",
    "# 4. Mark longer gaps as missing (for masking during training)\n",
    "\n",
    "MAX_FFILL_HOURS = 4\n",
    "MAX_INTERP_HOURS = 6\n",
    "\n",
    "def impute_vitals(group):\n",
    "    \"\"\"Impute missing values for a single ICU stay\"\"\"\n",
    "    df = group.copy()\n",
    "    \n",
    "    for col in VITAL_COLS:\n",
    "        # Forward fill (up to MAX_FFILL_HOURS)\n",
    "        df[col] = df[col].ffill(limit=MAX_FFILL_HOURS)\n",
    "        \n",
    "        # Backward fill (up to MAX_FFILL_HOURS)\n",
    "        df[col] = df[col].bfill(limit=MAX_FFILL_HOURS)\n",
    "        \n",
    "        # Linear interpolation (up to MAX_INTERP_HOURS)\n",
    "        df[col] = df[col].interpolate(method='linear', limit=MAX_INTERP_HOURS)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Imputing missing values...\")\n",
    "vitals_imputed = vitals_complete.groupby('ICUSTAY_ID', group_keys=False).apply(impute_vitals)\n",
    "\n",
    "# Check missingness after imputation\n",
    "print(\"\\nMissingness after imputation:\")\n",
    "for col in VITAL_COLS:\n",
    "    missing = vitals_imputed[col].isna().sum()\n",
    "    pct = missing / len(vitals_imputed) * 100\n",
    "    print(f\"  {col}: {missing:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating missingness masks...\n",
      "All missing values imputed\n"
     ]
    }
   ],
   "source": [
    "# Create missingness mask (for model training - to mask predictions at truly missing points)\n",
    "print(\"\\nCreating missingness masks...\")\n",
    "\n",
    "for col in VITAL_COLS:\n",
    "    vitals_imputed[f'{col}_MISSING'] = vitals_complete[col].isna().astype(int)\n",
    "\n",
    "# For remaining NaN after imputation, fill with population median\n",
    "for col in VITAL_COLS:\n",
    "    median_val = vitals_imputed[col].median()\n",
    "    vitals_imputed[col] = vitals_imputed[col].fillna(median_val)\n",
    "\n",
    "print(\"All missing values imputed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalization / Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NORMALIZATION\n",
      "============================================================\n",
      "Normalization parameters:\n",
      "        mean    std    min     max\n",
      "HR   107.934 35.783 20.000 254.000\n",
      "SpO2  97.447  2.425 50.000 100.000\n",
      "SBP  122.519 18.840 40.000 268.000\n",
      "DBP   60.019 11.689 20.000 200.000\n",
      "MBP   79.121 13.089 30.000 249.000\n",
      "RR    20.302  4.965  4.000  60.000\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"NORMALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store original values for later inverse transform\n",
    "vitals_original = vitals_imputed[VITAL_COLS].copy()\n",
    "\n",
    "# Calculate and store normalization parameters\n",
    "normalization_params = {}\n",
    "for col in VITAL_COLS:\n",
    "    normalization_params[col] = {\n",
    "        'mean': vitals_imputed[col].mean(),\n",
    "        'std': vitals_imputed[col].std(),\n",
    "        'min': vitals_imputed[col].min(),\n",
    "        'max': vitals_imputed[col].max()\n",
    "    }\n",
    "\n",
    "print(\"Normalization parameters:\")\n",
    "params_df = pd.DataFrame(normalization_params).T\n",
    "print(params_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Z-score normalization:\n",
      "              HR       SpO2        SBP        DBP        MBP         RR\n",
      "count 307210.000 307210.000 307210.000 307210.000 307210.000 307210.000\n",
      "mean      -0.000      0.000     -0.000     -0.000     -0.000     -0.000\n",
      "std        1.000      1.000      1.000      1.000      1.000      1.000\n",
      "min       -2.457    -19.568     -4.380     -3.424     -3.753     -3.283\n",
      "25%       -0.781     -0.184     -0.505     -0.429     -0.468     -0.464\n",
      "50%       -0.334      0.228     -0.081     -0.087     -0.086     -0.061\n",
      "75%        0.952      0.640      0.344      0.341      0.296      0.342\n",
      "max        4.082      1.053      7.722     11.976     12.978      7.995\n"
     ]
    }
   ],
   "source": [
    "# Apply Z-score normalization (StandardScaler)\n",
    "# Better for LSTM and gradient-based methods\n",
    "\n",
    "scaler = StandardScaler()\n",
    "vitals_imputed[VITAL_COLS] = scaler.fit_transform(vitals_imputed[VITAL_COLS])\n",
    "\n",
    "print(\"\\nAfter Z-score normalization:\")\n",
    "print(vitals_imputed[VITAL_COLS].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved to: C:\\Users\\hasin\\Downloads\\mimic-iii-clinical-database-1.4\\mimic-iii-clinical-database-1.4\\processed\\vital_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save scaler for later use (inverse transform predictions)\n",
    "import pickle\n",
    "\n",
    "scaler_path = f\"{OUTPUT_PATH}\\\\vital_scaler.pkl\"\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Also save normalization params as CSV for reference\n",
    "params_df.to_csv(f\"{OUTPUT_PATH}\\\\normalization_params.csv\")\n",
    "\n",
    "print(f\"Scaler saved to: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING\n",
      "============================================================\n",
      "Adding temporal features...\n",
      "\n",
      "Features added. New shape: (307210, 72)\n",
      "Total features: 72\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def add_temporal_features(df):\n",
    "    \"\"\"Add time-based features for each ICU stay\"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    for col in VITAL_COLS:\n",
    "        # Rolling statistics (past 6 hours)\n",
    "        result[f'{col}_roll_mean_6h'] = result.groupby('ICUSTAY_ID')[col].transform(\n",
    "            lambda x: x.rolling(window=6, min_periods=1).mean()\n",
    "        )\n",
    "        result[f'{col}_roll_std_6h'] = result.groupby('ICUSTAY_ID')[col].transform(\n",
    "            lambda x: x.rolling(window=6, min_periods=1).std()\n",
    "        )\n",
    "        result[f'{col}_roll_min_6h'] = result.groupby('ICUSTAY_ID')[col].transform(\n",
    "            lambda x: x.rolling(window=6, min_periods=1).min()\n",
    "        )\n",
    "        result[f'{col}_roll_max_6h'] = result.groupby('ICUSTAY_ID')[col].transform(\n",
    "            lambda x: x.rolling(window=6, min_periods=1).max()\n",
    "        )\n",
    "        \n",
    "        # Rate of change (difference from previous hour)\n",
    "        result[f'{col}_diff_1h'] = result.groupby('ICUSTAY_ID')[col].diff(1)\n",
    "        \n",
    "        # Rate of change (difference from 3 hours ago)\n",
    "        result[f'{col}_diff_3h'] = result.groupby('ICUSTAY_ID')[col].diff(3)\n",
    "        \n",
    "        # Lag features\n",
    "        result[f'{col}_lag_1h'] = result.groupby('ICUSTAY_ID')[col].shift(1)\n",
    "        result[f'{col}_lag_3h'] = result.groupby('ICUSTAY_ID')[col].shift(3)\n",
    "        result[f'{col}_lag_6h'] = result.groupby('ICUSTAY_ID')[col].shift(6)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Adding temporal features...\")\n",
    "vitals_featured = add_temporal_features(vitals_imputed)\n",
    "\n",
    "print(f\"\\nFeatures added. New shape: {vitals_featured.shape}\")\n",
    "print(f\"Total features: {len(vitals_featured.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding time-based features...\n",
      "Time-based features added\n"
     ]
    }
   ],
   "source": [
    "# Add time-based features\n",
    "print(\"\\nAdding time-based features...\")\n",
    "\n",
    "# Hours since ICU admission (already have this as 'HOUR')\n",
    "vitals_featured['hours_since_admission'] = vitals_featured['HOUR']\n",
    "\n",
    "# Normalized time in ICU (0 to 1 scale per patient)\n",
    "vitals_featured['time_normalized'] = vitals_featured.groupby('ICUSTAY_ID')['HOUR'].transform(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min() + 1)\n",
    ")\n",
    "\n",
    "# Is it early in ICU stay (first 24 hours)?\n",
    "vitals_featured['is_first_24h'] = (vitals_featured['HOUR'] < 24).astype(int)\n",
    "\n",
    "print(\"Time-based features added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filling NaN from feature engineering...\n",
      "Remaining NaN columns: 38\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN values created by rolling/lag operations\n",
    "print(\"\\nFilling NaN from feature engineering...\")\n",
    "\n",
    "# For rolling features, fill with the first available value\n",
    "vitals_featured = vitals_featured.groupby('ICUSTAY_ID', group_keys=False).apply(\n",
    "    lambda x: x.ffill().bfill()\n",
    ")\n",
    "\n",
    "# Check for remaining NaN\n",
    "nan_counts = vitals_featured.isna().sum()\n",
    "nan_cols = nan_counts[nan_counts > 0]\n",
    "if len(nan_cols) > 0:\n",
    "    print(f\"Remaining NaN columns: {len(nan_cols)}\")\n",
    "    # Fill with 0 for remaining\n",
    "    vitals_featured = vitals_featured.fillna(0)\n",
    "else:\n",
    "    print(\"No remaining NaN values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deterioration Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DETERIORATION LABELS\n",
      "============================================================\n",
      "Patients with hospital mortality: 11.4%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DETERIORATION LABELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Merge mortality information (only if not already merged)\n",
    "if 'HOSPITAL_EXPIRE_FLAG' not in vitals_featured.columns:\n",
    "    mortality_info = admissions[['HADM_ID', 'HOSPITAL_EXPIRE_FLAG', 'DEATHTIME']].drop_duplicates()\n",
    "    vitals_featured = vitals_featured.merge(mortality_info, on='HADM_ID', how='left')\n",
    "\n",
    "# Add ICU stay info for death time calculation (only if not already merged)\n",
    "if 'INTIME' not in vitals_featured.columns:\n",
    "    vitals_featured = vitals_featured.merge(\n",
    "        icustays[['ICUSTAY_ID', 'INTIME']].drop_duplicates(),\n",
    "        on='ICUSTAY_ID',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "print(f\"Patients with hospital mortality: {vitals_featured.groupby('ICUSTAY_ID')['HOSPITAL_EXPIRE_FLAG'].first().mean() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DETERIORATION LABELS\n",
      "============================================================\n",
      "\n",
      "Current vitals_featured shape: (307210, 84)\n",
      "Mortality info already merged, skipping...\n",
      "ICU stay info already merged, skipping...\n",
      "Shape after merges: (307210, 84)\n",
      "\n",
      "Mortality Statistics:\n",
      "  Total unique ICU stays: 1,977\n",
      "  Patients with hospital mortality: 226 (11.4%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DETERIORATION LABELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First, check current shape\n",
    "print(f\"\\nCurrent vitals_featured shape: {vitals_featured.shape}\")\n",
    "\n",
    "# Merge mortality information - but avoid duplicating rows\n",
    "mortality_info = admissions[['HADM_ID', 'HOSPITAL_EXPIRE_FLAG', 'DEATHTIME']].drop_duplicates()\n",
    "\n",
    "# Check if columns already exist (avoid re-merging)\n",
    "if 'HOSPITAL_EXPIRE_FLAG' not in vitals_featured.columns:\n",
    "    vitals_featured = vitals_featured.merge(mortality_info, on='HADM_ID', how='left')\n",
    "else:\n",
    "    print(\"Mortality info already merged, skipping...\")\n",
    "\n",
    "# Add ICU stay info for death time calculation\n",
    "if 'INTIME' not in vitals_featured.columns:\n",
    "    vitals_featured = vitals_featured.merge(\n",
    "        icustays[['ICUSTAY_ID', 'INTIME']].drop_duplicates(),\n",
    "        on='ICUSTAY_ID',\n",
    "        how='left'\n",
    "    )\n",
    "else:\n",
    "    print(\"ICU stay info already merged, skipping...\")\n",
    "\n",
    "print(f\"Shape after merges: {vitals_featured.shape}\")\n",
    "\n",
    "# Calculate mortality rate CORRECTLY\n",
    "# Count unique ICU stays with mortality flag = 1\n",
    "unique_stays = vitals_featured.drop_duplicates(subset=['ICUSTAY_ID'])\n",
    "mortality_count = unique_stays['HOSPITAL_EXPIRE_FLAG'].sum()\n",
    "total_stays = len(unique_stays)\n",
    "mortality_rate = mortality_count / total_stays * 100\n",
    "\n",
    "print(f\"\\nMortality Statistics:\")\n",
    "print(f\"  Total unique ICU stays: {total_stays:,}\")\n",
    "print(f\"  Patients with hospital mortality: {int(mortality_count):,} ({mortality_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vital sign deterioration:\n",
      "  Hours with 2+ critical vitals: 17,919 (5.83%)\n"
     ]
    }
   ],
   "source": [
    "# 3. Vital sign deterioration (threshold-based)\n",
    "# Flag hours where vitals are in critical ranges\n",
    "\n",
    "# Define critical thresholds (using normalized values - need to convert)\n",
    "# For normalized data, we'll use z-scores\n",
    "# Critical = more than 2 standard deviations from mean\n",
    "\n",
    "def create_vital_deterioration_flags(df):\n",
    "    \"\"\"Create flags for abnormal vital signs\"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # For normalized data: |z| > 2 is concerning\n",
    "    for col in VITAL_COLS:\n",
    "        result[f'{col}_critical'] = (abs(result[col]) > 2).astype(int)\n",
    "    \n",
    "    # Combined deterioration score (number of critical vitals)\n",
    "    critical_cols = [f'{col}_critical' for col in VITAL_COLS]\n",
    "    result['n_critical_vitals'] = result[critical_cols].sum(axis=1)\n",
    "    \n",
    "    # Binary deterioration flag (2+ critical vitals)\n",
    "    result['vital_deterioration'] = (result['n_critical_vitals'] >= 2).astype(int)\n",
    "    \n",
    "    return result\n",
    "\n",
    "vitals_featured = create_vital_deterioration_flags(vitals_featured)\n",
    "\n",
    "print(f\"\\nVital sign deterioration:\")\n",
    "print(f\"  Hours with 2+ critical vitals: {vitals_featured['vital_deterioration'].sum():,} ({vitals_featured['vital_deterioration'].mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Windowed Sequences for Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WINDOWED SEQUENCE CREATION\n",
      "============================================================\n",
      "\n",
      "Window configuration:\n",
      "  Input window: 6 hours\n",
      "  Forecast horizon: 6 hours\n",
      "  Total sequence length: 12 hours\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"WINDOWED SEQUENCE CREATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Forecasting configuration\n",
    "# Your proposal: 60-minute input → 240-minute forecast\n",
    "# With hourly data: 6-hour input → 4-hour (or more) forecast is reasonable\n",
    "\n",
    "INPUT_HOURS = 6      # 6 hours of historical data\n",
    "FORECAST_HOURS = 6   # Predict next 6 hours\n",
    "\n",
    "print(f\"\\nWindow configuration:\")\n",
    "print(f\"  Input window: {INPUT_HOURS} hours\")\n",
    "print(f\"  Forecast horizon: {FORECAST_HOURS} hours\")\n",
    "print(f\"  Total sequence length: {INPUT_HOURS + FORECAST_HOURS} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mortality_24h labels...\n",
      "\n",
      "Deterioration label distribution:\n",
      "  Mortality within 24h: 3,536 (1.15%)\n",
      "  Mortality within 48h: 6,854 (2.23%)\n"
     ]
    }
   ],
   "source": [
    "# CREATINg MORTALITY LABELS before sequence creation\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. Mortality within next 24 hours\n",
    "def label_mortality_24h(row):\n",
    "    if pd.isna(row['DEATHTIME']) or pd.isna(row['INTIME']):\n",
    "        return 0\n",
    "    try:\n",
    "        current_time = row['INTIME'] + timedelta(hours=row['HOUR'])\n",
    "        hours_to_death = (row['DEATHTIME'] - current_time).total_seconds() / 3600\n",
    "        return 1 if 0 < hours_to_death <= 24 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "print(\"Creating mortality_24h labels...\")\n",
    "vitals_featured['mortality_24h'] = vitals_featured.apply(label_mortality_24h, axis=1)\n",
    "\n",
    "# 2. Mortality within next 48 hours\n",
    "def label_mortality_48h(row):\n",
    "    if pd.isna(row['DEATHTIME']) or pd.isna(row['INTIME']):\n",
    "        return 0\n",
    "    try:\n",
    "        current_time = row['INTIME'] + timedelta(hours=row['HOUR'])\n",
    "        hours_to_death = (row['DEATHTIME'] - current_time).total_seconds() / 3600\n",
    "        return 1 if 0 < hours_to_death <= 48 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "vitals_featured['mortality_48h'] = vitals_featured.apply(label_mortality_48h, axis=1)\n",
    "\n",
    "print(f\"\\nDeterioration label distribution:\")\n",
    "print(f\"  Mortality within 24h: {vitals_featured['mortality_24h'].sum():,} ({vitals_featured['mortality_24h'].mean()*100:.2f}%)\")\n",
    "print(f\"  Mortality within 48h: {vitals_featured['mortality_48h'].sum():,} ({vitals_featured['mortality_48h'].mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vital sign deterioration:\n",
      "  Hours with 2+ critical vitals: 17,919 (5.83%)\n"
     ]
    }
   ],
   "source": [
    "# 3. Vital sign deterioration flags\n",
    "def create_vital_deterioration_flags(df):\n",
    "    result = df.copy()\n",
    "    \n",
    "    for col in VITAL_COLS:\n",
    "        if col in result.columns:\n",
    "            result[f'{col}_critical'] = (abs(result[col]) > 2).astype(int)\n",
    "    \n",
    "    critical_cols = [f'{col}_critical' for col in VITAL_COLS if f'{col}_critical' in result.columns]\n",
    "    result['n_critical_vitals'] = result[critical_cols].sum(axis=1)\n",
    "    result['vital_deterioration'] = (result['n_critical_vitals'] >= 2).astype(int)\n",
    "    \n",
    "    return result\n",
    "\n",
    "vitals_featured = create_vital_deterioration_flags(vitals_featured)\n",
    "\n",
    "print(f\"Vital sign deterioration:\")\n",
    "print(f\"  Hours with 2+ critical vitals: {vitals_featured['vital_deterioration'].sum():,} ({vitals_featured['vital_deterioration'].mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sequences...\n",
      "\n",
      "Sequences created:\n",
      "  X shape: (285482, 6, 26) (samples, input_hours, features)\n",
      "  y shape: (285482, 6, 6) (samples, forecast_hours, vitals)\n",
      "  Number of features: 26\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(df, input_hours, forecast_hours, vital_cols):\n",
    "    \"\"\"\n",
    "    Create input-output sequences for each ICU stay.\n",
    "    \n",
    "    Returns:\n",
    "        X: Input sequences (n_samples, input_hours, n_features)\n",
    "        y: Target sequences (n_samples, forecast_hours, n_vitals)\n",
    "        meta: Metadata (ICUSTAY_ID, start_hour, etc.)\n",
    "    \"\"\"\n",
    "    sequences_X = []\n",
    "    sequences_y = []\n",
    "    metadata = []\n",
    "    \n",
    "    # Features to use as input\n",
    "    feature_cols = vital_cols + [f'{v}_roll_mean_6h' for v in vital_cols] + \\\n",
    "                   [f'{v}_roll_std_6h' for v in vital_cols] + \\\n",
    "                   [f'{v}_diff_1h' for v in vital_cols] + \\\n",
    "                   ['hours_since_admission', 'time_normalized']\n",
    "    \n",
    "    # Filter to only existing columns\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "    \n",
    "    total_window = input_hours + forecast_hours\n",
    "    \n",
    "    for icustay_id in df['ICUSTAY_ID'].unique():\n",
    "        patient_data = df[df['ICUSTAY_ID'] == icustay_id].sort_values('HOUR')\n",
    "        \n",
    "        if len(patient_data) < total_window:\n",
    "            continue\n",
    "        \n",
    "        # Create sliding windows\n",
    "        for start_idx in range(len(patient_data) - total_window + 1):\n",
    "            # Input window\n",
    "            input_window = patient_data.iloc[start_idx:start_idx + input_hours]\n",
    "            # Target window\n",
    "            target_window = patient_data.iloc[start_idx + input_hours:start_idx + total_window]\n",
    "            \n",
    "            # Extract features and targets\n",
    "            X = input_window[feature_cols].values\n",
    "            y = target_window[vital_cols].values\n",
    "            \n",
    "            sequences_X.append(X)\n",
    "            sequences_y.append(y)\n",
    "            metadata.append({\n",
    "                'ICUSTAY_ID': icustay_id,\n",
    "                'start_hour': input_window['HOUR'].iloc[0],\n",
    "                'mortality_in_window': target_window['mortality_24h'].max()\n",
    "            })\n",
    "    \n",
    "    return np.array(sequences_X), np.array(sequences_y), pd.DataFrame(metadata), feature_cols\n",
    "\n",
    "print(\"Creating sequences...\")\n",
    "X, y, meta, feature_names = create_sequences(\n",
    "    vitals_featured, \n",
    "    INPUT_HOURS, \n",
    "    FORECAST_HOURS, \n",
    "    VITAL_COLS\n",
    ")\n",
    "\n",
    "print(f\"\\nSequences created:\")\n",
    "print(f\"  X shape: {X.shape} (samples, input_hours, features)\")\n",
    "print(f\"  y shape: {y.shape} (samples, forecast_hours, vitals)\")\n",
    "print(f\"  Number of features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature names:\n",
      "  0: HR\n",
      "  1: SpO2\n",
      "  2: SBP\n",
      "  3: DBP\n",
      "  4: MBP\n",
      "  5: RR\n",
      "  6: Temp\n",
      "  7: HR_roll_mean_6h\n",
      "  8: SpO2_roll_mean_6h\n",
      "  9: SBP_roll_mean_6h\n",
      "  10: DBP_roll_mean_6h\n",
      "  11: MBP_roll_mean_6h\n",
      "  12: RR_roll_mean_6h\n",
      "  13: Temp_roll_mean_6h\n",
      "  14: HR_roll_std_6h\n",
      "  15: SpO2_roll_std_6h\n",
      "  16: SBP_roll_std_6h\n",
      "  17: DBP_roll_std_6h\n",
      "  18: MBP_roll_std_6h\n",
      "  19: RR_roll_std_6h\n",
      "  20: Temp_roll_std_6h\n",
      "  21: HR_diff_1h\n",
      "  22: SpO2_diff_1h\n",
      "  23: SBP_diff_1h\n",
      "  24: DBP_diff_1h\n",
      "  25: MBP_diff_1h\n",
      "  26: RR_diff_1h\n",
      "  27: Temp_diff_1h\n",
      "  28: hours_since_admission\n",
      "  29: time_normalized\n"
     ]
    }
   ],
   "source": [
    "# Save feature names for later use\n",
    "print(\"\\nFeature names:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAIN/VALIDATION/TEST SPLIT\n",
      "============================================================\n",
      "\n",
      "Total unique ICU stays: 1973\n",
      "  Train stays: 1381\n",
      "  Validation stays: 296\n",
      "  Test stays: 296\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAIN/VALIDATION/TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split by ICU stay (not by individual sequences) to prevent data leakage\n",
    "unique_stays = meta['ICUSTAY_ID'].unique()\n",
    "print(f\"\\nTotal unique ICU stays: {len(unique_stays)}\")\n",
    "\n",
    "# 70% train, 15% validation, 15% test\n",
    "train_stays, temp_stays = train_test_split(unique_stays, test_size=0.30, random_state=42)\n",
    "val_stays, test_stays = train_test_split(temp_stays, test_size=0.50, random_state=42)\n",
    "\n",
    "print(f\"  Train stays: {len(train_stays)}\")\n",
    "print(f\"  Validation stays: {len(val_stays)}\")\n",
    "print(f\"  Test stays: {len(test_stays)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence counts:\n",
      "  Train: 203,407 sequences\n",
      "  Validation: 35,151 sequences\n",
      "  Test: 46,924 sequences\n"
     ]
    }
   ],
   "source": [
    "# Create boolean masks\n",
    "train_mask = meta['ICUSTAY_ID'].isin(train_stays)\n",
    "val_mask = meta['ICUSTAY_ID'].isin(val_stays)\n",
    "test_mask = meta['ICUSTAY_ID'].isin(test_stays)\n",
    "\n",
    "# Split data\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_val, y_val = X[val_mask], y[val_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "meta_train = meta[train_mask].reset_index(drop=True)\n",
    "meta_val = meta[val_mask].reset_index(drop=True)\n",
    "meta_test = meta[test_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nSequence counts:\")\n",
    "print(f\"  Train: {len(X_train):,} sequences\")\n",
    "print(f\"  Validation: {len(X_val):,} sequences\")\n",
    "print(f\"  Test: {len(X_test):,} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data leakage detected - splits are clean\n"
     ]
    }
   ],
   "source": [
    "# Verify no data leakage\n",
    "train_set = set(meta_train['ICUSTAY_ID'])\n",
    "val_set = set(meta_val['ICUSTAY_ID'])\n",
    "test_set = set(meta_test['ICUSTAY_ID'])\n",
    "\n",
    "assert len(train_set & val_set) == 0, \"Data leakage: train and val overlap!\"\n",
    "assert len(train_set & test_set) == 0, \"Data leakage: train and test overlap!\"\n",
    "assert len(val_set & test_set) == 0, \"Data leakage: val and test overlap!\"\n",
    "\n",
    "print(\"No data leakage detected - splits are clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING PROCESSED DATA\n",
      "============================================================\n",
      "Numpy arrays saved\n",
      "Metadata saved\n",
      "Feature names saved\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save as numpy arrays (for LSTM)\n",
    "np.save(f\"{OUTPUT_PATH}\\\\X_train.npy\", X_train)\n",
    "np.save(f\"{OUTPUT_PATH}\\\\y_train.npy\", y_train)\n",
    "np.save(f\"{OUTPUT_PATH}\\\\X_val.npy\", X_val)\n",
    "np.save(f\"{OUTPUT_PATH}\\\\y_val.npy\", y_val)\n",
    "np.save(f\"{OUTPUT_PATH}\\\\X_test.npy\", X_test)\n",
    "np.save(f\"{OUTPUT_PATH}\\\\y_test.npy\", y_test)\n",
    "\n",
    "print(\"Numpy arrays saved\")\n",
    "\n",
    "# Save metadata\n",
    "meta_train.to_csv(f\"{OUTPUT_PATH}\\\\meta_train.csv\", index=False)\n",
    "meta_val.to_csv(f\"{OUTPUT_PATH}\\\\meta_val.csv\", index=False)\n",
    "meta_test.to_csv(f\"{OUTPUT_PATH}\\\\meta_test.csv\", index=False)\n",
    "\n",
    "print(\"Metadata saved\")\n",
    "\n",
    "# Save feature names\n",
    "pd.DataFrame({'feature': feature_names}).to_csv(f\"{OUTPUT_PATH}\\\\feature_names.csv\", index=False)\n",
    "pd.DataFrame({'vital': VITAL_COLS}).to_csv(f\"{OUTPUT_PATH}\\\\vital_names.csv\", index=False)\n",
    "\n",
    "print(\"Feature names saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full processed dataset saved: 307,210 rows\n",
      "Configuration saved\n"
     ]
    }
   ],
   "source": [
    "# Save the full processed dataset (for ARIMA and other analyses)\n",
    "vitals_featured.to_csv(f\"{OUTPUT_PATH}\\\\vitals_processed_full.csv\", index=False)\n",
    "print(f\"Full processed dataset saved: {len(vitals_featured):,} rows\")\n",
    "\n",
    "# Save configuration\n",
    "config = {\n",
    "    'INPUT_HOURS': INPUT_HOURS,\n",
    "    'FORECAST_HOURS': FORECAST_HOURS,\n",
    "    'VITAL_COLS': VITAL_COLS,\n",
    "    'N_FEATURES': len(feature_names),\n",
    "    'N_TRAIN': len(X_train),\n",
    "    'N_VAL': len(X_val),\n",
    "    'N_TEST': len(X_test)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"{OUTPUT_PATH}\\\\config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Configuration saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREPROCESSING COMPLETE - SUMMARY\n",
      "============================================================\n",
      "\n",
      "Data Configuration:\n",
      "  - Input window: 6 hours\n",
      "  - Forecast horizon: 6 hours\n",
      "  - Vital signs: ['HR', 'SpO2', 'SBP', 'DBP', 'MBP', 'RR']\n",
      "  - Total features: 26\n",
      "\n",
      "Dataset Sizes:\n",
      "  - Train: 203,407 sequences from 1381 patients\n",
      "  - Validation: 35,151 sequences from 296 patients\n",
      "  - Test: 46,924 sequences from 296 patients\n",
      "\n",
      "Array Shapes:\n",
      "  - X: (n_samples, 6, 26) \n",
      "  - y: (n_samples, 6, 6)\n",
      "\n",
      "Output Files Saved to: C:\\Users\\hasin\\Downloads\\mimic-iii-clinical-database-1.4\\mimic-iii-clinical-database-1.4\\processed\n",
      "  - X_train.npy, y_train.npy\n",
      "  - X_val.npy, y_val.npy  \n",
      "  - X_test.npy, y_test.npy\n",
      "  - meta_train.csv, meta_val.csv, meta_test.csv\n",
      "  - vital_scaler.pkl (for inverse transform)\n",
      "  - vitals_processed_full.csv (for ARIMA)\n",
      "  - config.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Data Configuration:\n",
    "  - Input window: {INPUT_HOURS} hours\n",
    "  - Forecast horizon: {FORECAST_HOURS} hours\n",
    "  - Vital signs: {VITAL_COLS}\n",
    "  - Total features: {len(feature_names)}\n",
    "\n",
    "Dataset Sizes:\n",
    "  - Train: {len(X_train):,} sequences from {len(train_stays)} patients\n",
    "  - Validation: {len(X_val):,} sequences from {len(val_stays)} patients\n",
    "  - Test: {len(X_test):,} sequences from {len(test_stays)} patients\n",
    "\n",
    "Array Shapes:\n",
    "  - X: (n_samples, {INPUT_HOURS}, {len(feature_names)}) \n",
    "  - y: (n_samples, {FORECAST_HOURS}, {len(VITAL_COLS)})\n",
    "\n",
    "Output Files Saved to: {OUTPUT_PATH}\n",
    "  - X_train.npy, y_train.npy\n",
    "  - X_val.npy, y_val.npy  \n",
    "  - X_test.npy, y_test.npy\n",
    "  - meta_train.csv, meta_val.csv, meta_test.csv\n",
    "  - vital_scaler.pkl (for inverse transform)\n",
    "  - vitals_processed_full.csv (for ARIMA)\n",
    "  - config.json\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Loading Template (for next notebooks)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "```\n",
    "## INSERT PATH FROM YOUR DESTINATION here\n",
    "```python\n",
    "OUTPUT_PATH = r\"C:\\Users\\hasin\\Downloads\\mimic-iii-clinical-database-1.4\\mimic-iii-clinical-database-1.4\\processed\"\n",
    "\n",
    "# Load config\n",
    "with open(f\"{OUTPUT_PATH}\\\\config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load sequences\n",
    "X_train = np.load(f\"{OUTPUT_PATH}\\\\X_train.npy\")\n",
    "y_train = np.load(f\"{OUTPUT_PATH}\\\\y_train.npy\")\n",
    "X_val = np.load(f\"{OUTPUT_PATH}\\\\X_val.npy\")\n",
    "y_val = np.load(f\"{OUTPUT_PATH}\\\\y_val.npy\")\n",
    "X_test = np.load(f\"{OUTPUT_PATH}\\\\X_test.npy\")\n",
    "y_test = np.load(f\"{OUTPUT_PATH}\\\\y_test.npy\")\n",
    "\n",
    "# Load scaler (for inverse transform)\n",
    "with open(f\"{OUTPUT_PATH}\\\\vital_scaler.pkl\", 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "# Load feature names\n",
    "feature_names = pd.read_csv(f\"{OUTPUT_PATH}\\\\feature_names.csv\")['feature'].tolist()\n",
    "vital_names = pd.read_csv(f\"{OUTPUT_PATH}\\\\vital_names.csv\")['vital'].tolist()\n",
    "\n",
    "print(f\"Loaded: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
